{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolocassina/Principles-of-Machine-Learning-Python/blob/master/TP01_MLP_vs_toy_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxQ13chKKnT"
      },
      "source": [
        "# TP01 - Training and visualizing MLPs on toy datasets\n",
        "\n",
        "In this practical, we are going to learn how to train a simple feed forward neural network (aka Multi-Layer Perceptron or MLP) on a synthetic dataset and visualize the result. We will use a custom package to help automatically check your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkQ5hy55K8jG"
      },
      "source": [
        "### Auto-checker setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4KtyY-lJVCP"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade \"git+https://gitlab.com/robindar/dl-scaman_checker.git\"\n",
        "from dl_scaman_checker import TP01\n",
        "TP01.check_install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uFnHJm8K8jH"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krX3ndDIKojY"
      },
      "source": [
        "First import the three libraries we will use: **torch**, **numpy** (as np) and **matplotlib.pyplot** (as plt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKEOv_EsK8jH"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "TP01.check_imports()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj5UoUBrK-Xf"
      },
      "source": [
        "# Part A - Checkerboard learning\n",
        "\n",
        "## A.0 - Data generation\n",
        "\n",
        "Then, create a function `create_checkerboard` that returns two pytorch Tensors: `X` of shape $(100,2)$ where $100$ is the number of data points and each point is drawn uniformly at random in $[-2,2]^2$, and `y` of shape $(100,2)$ where $y_i=(1,0)$ if $X_{i,1} \\cdot X_{i,2} > 0$, and $y_i=(0,1)$ otherwise. The vectors $y_i$ are called **one-hot** encodings of the classes 0 and 1.\n",
        "\n",
        "**IMPORTANT:** Always test the shape of your tensor with `print(X.shape)` to verify that your are computing the right quantity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOoApLjcJXdn"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Z6Z_kmK8jH"
      },
      "outputs": [],
      "source": [
        "X, y = create_checkerboard()\n",
        "\n",
        "TP01.check_dimensions(X, y)\n",
        "TP01.check_distributions(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbMcop8KK8jH"
      },
      "outputs": [],
      "source": [
        "# Visualizing the dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y[:,0])\n",
        "plt.xlabel(\"$X_1$\")\n",
        "plt.ylabel(\"$X_2$\")\n",
        "plt.title(\"Checkerboard dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2C2TRFST99w"
      },
      "source": [
        "## A.1 - Model creation\n",
        "\n",
        "First, let's create a two-layer perceptron with ReLU activations.\n",
        "Create a function `create_simple_mlp(width)` that returns an MLP with 2 inputs, 2 outputs, and `width` internal neurons.\n",
        "Check the documentation for `torch.nn.Sequential`, used to construct MLPs.\n",
        "Since we are using linear layers and a ReLU activation, you will also need\n",
        "`torch.nn.Linear` and `torch.nn.ReLU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G453zpDYUuF_"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `print` and `shape`, verify that the model outputs a 2-dimensional vector for each input data point on the checkerboard dataset."
      ],
      "metadata": {
        "id": "6Rl9vCnbtI94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gojs3BbLK8jI"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "TP01.check_model_signature((X,y), model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD1ycoGwOgI_"
      },
      "source": [
        "## A.2 - Training pipeline\n",
        "\n",
        "### A.2.0 - Visualizing network output\n",
        "\n",
        "We will visualize the network output as follows. For now, the visualize function is provided. Later, you will write your own.\n",
        "On the left, we plot for each point of the input plane the difference between the predicted coordinates $y_{i,0} - y_{i,1}$, positive\n",
        "if the network classifies the point as class 0, and negative if it is classified as class 1.\n",
        "On the right, we plot the evolution of the loss over training, given by the `losses` array. You will populate this array when writing your training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PF30ZW9JpZj"
      },
      "outputs": [],
      "source": [
        "example_losses = [1/i for i in range(1,10)]\n",
        "TP01.visualize(data=(X,y), model=model, epoch=0, losses=example_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4AlxC1tK8jI"
      },
      "source": [
        "### A.2.1 - Computing gradients and modifying weights\n",
        "\n",
        "In PyTorch, a model automatically computes the gradients with respect to a given computation whenever the `.backward()` method is called.\n",
        "Here is an example of how to access the models and their parameters. After computing a value and calling `value.backward()`, the gradient of the model parameters is automatically available with the `.grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJBWhXlsK8jI"
      },
      "outputs": [],
      "source": [
        "[ p.shape for p in model.parameters() ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ynUr5qaK8jI"
      },
      "outputs": [],
      "source": [
        "p = next(model.parameters())\n",
        "print(p.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbEVQD9UK8jI"
      },
      "outputs": [],
      "source": [
        "value = (model(X) ** 2).mean()\n",
        "value.backward()\n",
        "print(p.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOmMT6qOK8jJ"
      },
      "source": [
        "One can then reset the gradients with the method `.zero_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhQEDEChK8jJ"
      },
      "outputs": [],
      "source": [
        "model.zero_grad()\n",
        "print(p.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5wuXOEIP6A_"
      },
      "source": [
        "### A.2.2 - Writing the gradient descent\n",
        "\n",
        "Our objective is to push the model, for an input `X` to return an output that matches `y`. To do so we will minimize the **mean square error** $MSE = \\frac{1}{N}\\sum_{i=1}^N \\|\\mbox{model}(X_i) - y_i\\|^2$ over the entire dataset using gradient descent.\n",
        "\n",
        "Create a function `train(data, model, epochs, visualize=True)` that takes a dataset of inputs and outputs `data`, a neural network `model`, and trains the model for `epoch` epochs. For each epoch, you will need to, in order:\n",
        "\n",
        "1. Prepare gradient computations using `model.zero_grad()`\n",
        "2. Compute the output of the `model` over the input data `X`\n",
        "3. Compute `loss`, the mean squared error loss over the entire dataset.\n",
        "4. Use `loss.backward()` to automatically compute the gradients of the loss wrt the model parameters.\n",
        "5. Make one step of gradient descent by updating the tensor `p.data` of each parameter `p` in `model.parameters()` with the formula $p = p - \\eta \\nabla L(p)$ where $\\eta=10^{-2}$ is the step-size and $L$ is the loss. Recall that the gradient computed by `loss.backward()` is accessible in `p.grad`.\n",
        "  \n",
        "If `vizualize` is set to `True`, visualize the output of the neural network and loss using `TP01.visualize` once every 1000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUWp4-aEJll_"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBfCk-QOVy8K"
      },
      "source": [
        "Finally, train your MLP on the checkerboard dataset for 10000 epochs.\n",
        "You should observe a decreasing loss, which reaches approximatively $6 \\cdot 10^{-2}$ after 10000 epochs (the precise value varies, and depends on the random initialization, but the order of magnitude is usually similar).\n",
        "You can get to lower loss values by using more training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPwqBkKZK8jJ"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EZV201dPrQ6"
      },
      "source": [
        "Is it predicting the correct classes? Does the prediction line up with the checkerboard pattern you expected ?\n",
        "Try with 2, 10, and 100 neurons in the internal layer. See how the predictions change, and how close to zero the loss gets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqqj0pYIK8jJ"
      },
      "source": [
        "## A.3 - Measuring accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ptxr7FYK8jJ"
      },
      "source": [
        "In order to more efficiently evalute the differences observed when changing the size of the internal layer,\n",
        "write an accuracy method which computes the mean accuracy of a model. Recall that for an output $y \\in \\mathbb{R}^2$,\n",
        "we say that the model predicts 'class 0' if $y_0 > y_1$ and that it predicts 'class 1' otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6pkrQk4K8jJ"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy((X,y), model)\n",
        "TP01.check_accuracy((X,y), model, acc)"
      ],
      "metadata": {
        "id": "rjfs4XoZbtDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9slBG-q6K8jK"
      },
      "source": [
        "## A.4 - Impact of width\n",
        "\n",
        "Do larger models tend to perform better, the same, or worse ? Let us check the training accuracy of models as a function of the width. For each width in the list `[2, 5, 10, 20, 50, 100, 200, 1000]`, train a model of the specified width for 10,000 epochs, and record its final training accuracy. Plot the training accuracy with respect to the width with a log scale using `plt.xscale('log')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oObYfLZK8jK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvtYmh9VK8jN"
      },
      "source": [
        "### A.5 - Write your own visualization (optional)\n",
        "\n",
        "You have been using the provided visualization code to check your model's output. Now, write your own `visualize(data, model, epoch, losses)` !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWwDSoSuK8jN"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYkb5fgWCup"
      },
      "source": [
        "# Part B - Harder dataset\n",
        "\n",
        "## B.0 - Data creation\n",
        "\n",
        "The following code create a more challenging dataset consisting of two intertwined spirals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojjFvkCjQJst"
      },
      "outputs": [],
      "source": [
        "def create_spiral():\n",
        "    # Creates a spiral\n",
        "    theta = np.linspace(0, 4 * np.pi, 100)\n",
        "    r = np.linspace(0.5, 2, 100)\n",
        "    x = r * np.cos(theta)\n",
        "    y = r * np.sin(theta)\n",
        "\n",
        "    # Creates another spiral for the second class\n",
        "    theta = np.linspace(0, 4 * np.pi, 100)\n",
        "    r = np.linspace(0.5, 2, 100) + 0.3\n",
        "    x2 = r * np.cos(theta)\n",
        "    y2 = r * np.sin(theta)\n",
        "\n",
        "    # Merging both spirals into a single dataset\n",
        "    X = np.vstack((np.hstack((x, x2)), np.hstack((y, y2)))).T\n",
        "    y = np.hstack((np.zeros(100), np.ones(100)))\n",
        "\n",
        "    # From numpy arrays to Pytorch tensors\n",
        "    X = torch.FloatTensor(X)\n",
        "    y = torch.LongTensor(np.stack([y,1-y], axis=-1))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Visualizing the dataset\n",
        "X, y = create_spiral()\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y[:,0])\n",
        "plt.xlabel(\"$X_1$\")\n",
        "plt.ylabel(\"$X_2$\")\n",
        "plt.title(\"Spiral dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDX8nmPhWN3J"
      },
      "source": [
        "## B.1 - Model creation, training, validation\n",
        "\n",
        "Train your model (don't forget to create a new model to reinitialize the parameters) on the spiral dataset for 50,000 epochs. Is it predicting the correct classes? Try increasing the number of neurons to 100 first, then more if the result does not convince you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l--6uvUVtoK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiS4q5RBK8jO"
      },
      "source": [
        "## B.2 - Varying model size and training hyperparameters\n",
        "\n",
        "Repeat the experiment with varying width, to check how the accuracy improves, or whether it does.\n",
        "You can also try to increase the number of training epochs for each width.\n",
        "Think about the computation time, and thus how many widths you want to try and for how long you want to train each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa4awJKiK8jO"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10jy1nkK8jO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}