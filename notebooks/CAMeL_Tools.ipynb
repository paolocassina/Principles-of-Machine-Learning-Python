{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolocassina/Principles-of-Machine-Learning-Python/blob/master/notebooks/CAMeL_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2oOVmkJgg-"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook provides a quick overview of the functionalities provided by [CAMeL Tools](https://github.com/CAMeL-Lab/camel_tools). While it doesn't cover every component in detail, it does provide a starting point for learning CAMeL Tools and navigating its APIs. For more detailed information please refer to the [CAMeL Tools documentation](https://camel-tools.readthedocs.io/en/latest/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOwO9Z7VdWf-"
      },
      "source": [
        "# Arabic NLP Refresher\n",
        "\n",
        "Before diving into CAMeL Tools, we recommend a quick refresher on Arabic NLP and the challenges it presents. [This webinar](https://wti.kaust.edu.sa/upcoming-events/Machine-Learning-Arabic-NLP-Webinar/ml-webinar-2020-keynote-1) presented by Dr. Nizar Habash provides a short introcuction on Arabic NLP as well as short examples of how CAMeL Tools can be used to solve various problems. A similar presentation of this webinar in Arabic is also available [here](https://www.youtube.com/watch?v=JQrmRvtx_VY).\n",
        "\n",
        "*Note: The examples in this talk may not be compatible with current or newer versions of CAMeL Tools.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp2VnX-14Kz"
      },
      "source": [
        "# Installation and Setup\n",
        "\n",
        "The following steps are needed if you want to run the examples in this notebook on Google Colaboratory. If you want to run this notebook on your own machine, please follow the [installation instructions](https://camel-tools.readthedocs.io/en/latest/getting_started.html#installation) instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47wgvS4-6UOD"
      },
      "source": [
        "First, we install the CAMeL Tools Python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLCWm2nj2LX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe11ec6-4953-43a1-d9f4-d63652b3cb0f"
      },
      "source": [
        "%pip install camel-tools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.5.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
            "Collecting docopt (from camel-tools)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
            "Collecting numpy<2 (from camel-tools)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.6.1)\n",
            "Collecting dill (from camel-tools)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.6.0+cu124)\n",
            "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\n",
            "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.32.3)\n",
            "Collecting emoji (from camel-tools)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyrsistent (from camel-tools)\n",
            "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.67.1)\n",
            "Collecting muddler (from camel-tools)\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting camel-kenlm>=2024.5.6 (from camel-tools)\n",
            "  Downloading camel-kenlm-2024.5.6.zip (556 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->camel-tools)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Downloading camel_tools-1.5.5-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m850.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: camel-kenlm, docopt\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2024.5.6-cp311-cp311-linux_x86_64.whl size=3186885 sha256=08af7ec67cdcce044fde1da5f4245bebc1eb9a56da71db8993b8d3b3da532d1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/a6/a5/4f25e9750d0cddf53013b69f0e4d18930a2f89a78fc6019c97\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=df9dd2b2189800901d995eedd62f95750b327622789a99208a81aa520ebb0750\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built camel-kenlm docopt\n",
            "Installing collected packages: docopt, camel-kenlm, pyrsistent, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, muddler, emoji, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, camel-tools\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "Successfully installed camel-kenlm-2024.5.6 camel-tools-1.5.5 dill-0.3.9 docopt-0.6.2 emoji-2.14.1 muddler-0.1.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8-zs9m45B0"
      },
      "source": [
        "In order to use all the components provided in CAMeL Tools, we need to install all the datasets required by these components.\n",
        "To do this in Colab, we need to first mount a Google Drive and create a directory where the data will be installed.\n",
        "\n",
        "Run the code below and follow the instructions in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLDEdYgz1OZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387514e8-751b-4dd8-f9c2-c489ed54cc1e"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcc0ZaAK5f5F"
      },
      "source": [
        "Next, we need to tell CAMeL Tools to install the data in the newly created directory. This will take a couple of minutes to complete.\n",
        "\n",
        "**NOTE:** You will need at least 2.3GB of available space on your Google Drive to install all the CAMeL Tools data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYsFJl0A5mua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d2cb10-5752-40d6-c270-d10fc114b564"
      },
      "source": [
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n",
        "!export | camel_data -i all"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following packages will be installed: 'disambig-bert-unfactored-msa', 'disambig-ranking-cache-calima-lev-01', 'morphology-db-glf-01', 'morphology-db-egy-r13', 'ner-arabert', 'morphology-db-lev-01', 'morphology-db-msa-r13', 'disambig-bert-unfactored-lev', 'disambig-bert-unfactored-egy', 'sentiment-analysis-arabert', 'disambig-ranking-cache-calima-msa-r13', 'sentiment-analysis-mbert', 'disambig-ranking-cache-calima-egy-r13', 'morphology-db-msa-s31', 'disambig-mle-calima-msa-r13', 'disambig-bert-unfactored-glf', 'dialectid-model6', 'disambig-ranking-cache-calima-glf-01', 'disambig-mle-calima-egy-r13', 'dialectid-model26'\n",
            "Downloading package 'disambig-bert-unfactored-msa': 100% 445M/445M [00:05<00:00, 84.5MB/s]\n",
            "Extracting package 'disambig-bert-unfactored-msa': 100% 445M/445M [00:05<00:00, 75.0MB/s]\n",
            "Downloading package 'disambig-ranking-cache-calima-lev-01': 100% 23.2M/23.2M [00:00<00:00, 149MB/s] \n",
            "Extracting package 'disambig-ranking-cache-calima-lev-01': 100% 23.2M/23.2M [00:00<00:00, 103MB/s]\n",
            "Downloading package 'morphology-db-glf-01': 100% 7.98M/7.98M [00:00<00:00, 77.6MB/s]\n",
            "Extracting package 'morphology-db-glf-01': 100% 7.98M/7.98M [00:00<00:00, 114MB/s]\n",
            "Downloading package 'morphology-db-egy-r13': 100% 67.3M/67.3M [00:00<00:00, 163MB/s]\n",
            "Extracting package 'morphology-db-egy-r13': 100% 67.3M/67.3M [00:00<00:00, 139MB/s]\n",
            "Downloading package 'ner-arabert': 100% 542M/542M [00:04<00:00, 129MB/s]\n",
            "Extracting package 'ner-arabert': 100% 542M/542M [00:08<00:00, 66.4MB/s]\n",
            "Downloading package 'morphology-db-lev-01': 100% 10.6M/10.6M [00:00<00:00, 100MB/s]\n",
            "Extracting package 'morphology-db-lev-01': 100% 10.6M/10.6M [00:00<00:00, 122MB/s]\n",
            "Downloading package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 155MB/s]\n",
            "Extracting package 'morphology-db-msa-r13': 100% 40.5M/40.5M [00:00<00:00, 95.0MB/s]\n",
            "Downloading package 'disambig-bert-unfactored-lev': 100% 441M/441M [00:05<00:00, 81.1MB/s]\n",
            "Extracting package 'disambig-bert-unfactored-lev': 100% 441M/441M [00:07<00:00, 61.0MB/s]\n",
            "Downloading package 'disambig-bert-unfactored-egy': 100% 446M/446M [00:02<00:00, 165MB/s]\n",
            "Extracting package 'disambig-bert-unfactored-egy': 100% 446M/446M [00:05<00:00, 88.2MB/s]\n",
            "Downloading package 'sentiment-analysis-arabert': 100% 542M/542M [00:05<00:00, 96.2MB/s]\n",
            "Extracting package 'sentiment-analysis-arabert': 100% 542M/542M [00:07<00:00, 76.9MB/s]\n",
            "Downloading package 'disambig-ranking-cache-calima-msa-r13': 100% 556M/556M [00:04<00:00, 119MB/s] \n",
            "Extracting package 'disambig-ranking-cache-calima-msa-r13': 100% 556M/556M [00:08<00:00, 69.5MB/s]\n",
            "Downloading package 'sentiment-analysis-mbert': 100% 712M/712M [00:06<00:00, 114MB/s] \n",
            "Extracting package 'sentiment-analysis-mbert': 100% 712M/712M [00:10<00:00, 70.8MB/s]\n",
            "Downloading package 'disambig-ranking-cache-calima-egy-r13': 100% 320M/320M [00:01<00:00, 180MB/s]\n",
            "Extracting package 'disambig-ranking-cache-calima-egy-r13': 100% 320M/320M [00:05<00:00, 54.6MB/s]\n",
            "Downloading package 'morphology-db-msa-s31': 100% 44.8M/44.8M [00:01<00:00, 43.3MB/s]\n",
            "Extracting package 'morphology-db-msa-s31': 100% 44.8M/44.8M [00:01<00:00, 34.5MB/s]\n",
            "Downloading package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:01<00:00, 64.8MB/s]\n",
            "Extracting package 'disambig-mle-calima-msa-r13': 100% 88.7M/88.7M [00:01<00:00, 54.8MB/s]\n",
            "Downloading package 'disambig-bert-unfactored-glf': 100% 442M/442M [00:04<00:00, 102MB/s]\n",
            "Extracting package 'disambig-bert-unfactored-glf': 100% 442M/442M [00:03<00:00, 113MB/s]\n",
            "Downloading package 'dialectid-model6': 100% 153M/153M [00:01<00:00, 95.3MB/s]\n",
            "Extracting package 'dialectid-model6': 100% 153M/153M [00:01<00:00, 106MB/s] \n",
            "Downloading package 'disambig-ranking-cache-calima-glf-01': 100% 28.7M/28.7M [00:00<00:00, 141MB/s]\n",
            "Extracting package 'disambig-ranking-cache-calima-glf-01': 100% 28.7M/28.7M [00:00<00:00, 137MB/s]\n",
            "Downloading package 'disambig-mle-calima-egy-r13': 100% 27.2M/27.2M [00:00<00:00, 137MB/s] \n",
            "Extracting package 'disambig-mle-calima-egy-r13': 100% 27.2M/27.2M [00:00<00:00, 106MB/s]\n",
            "Downloading package 'dialectid-model26': 100% 371M/371M [00:03<00:00, 109MB/s]\n",
            "Extracting package 'dialectid-model26': 100% 371M/371M [00:05<00:00, 72.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zZvcWRx6tQJ"
      },
      "source": [
        "We also provide a lightweight dataset for the Morphology and Disambiguation components **only** that can be installed by calling `camel_data -i light` instead of `camel_data -i all`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-O0aoMczyUj"
      },
      "source": [
        "**Once the data has been installed on your Google Drive, you only need to run the following the next time you want to run this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knGLLGg7cnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87cf5aca-f22b-45ef-b10b-5052f63de438"
      },
      "source": [
        "%pip install camel-tools\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: camel-tools in /usr/local/lib/python3.11/dist-packages (1.5.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.3.9)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers<4.44.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.43.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.32.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.14.1)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.20.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from camel-tools) (4.67.1)\n",
            "Requirement already satisfied: muddler in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.1.3)\n",
            "Requirement already satisfied: camel-kenlm>=2024.5.6 in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2024.5.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->camel-tools) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->camel-tools) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.19.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->camel-tools) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->camel-tools) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->camel-tools) (3.0.2)\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2mkVx2f088f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBNnYjuE9T_6"
      },
      "source": [
        "CAMeL Tools provides a suite of preprocessing utilities for cleaning and preparing Arabic text. Some of these may need to be used before passing text to other CAMeL Tools components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-V9yJNU088w"
      },
      "source": [
        "## Simple Transliteration\n",
        "\n",
        "Simple (or one-to-one) transliteration is the process of translating each Arabic character to/from a single non-Arabic character mapping (or encoding). The most common of these encoding schemes is the Buckwalter scheme and its variants that map Arabic characters to ASCII counterparts. You can see the various encoding schemes supported in CAMeL Tools and how they map to Arabic script [here](https://camel-tools.readthedocs.io/en/latest/reference/encoding_schemes.html).\n",
        "\n",
        "This has many uses including:\n",
        "\n",
        "- Smaller size of Arabic text on disk\n",
        "- Allow non-Arabic speakers and people not familiar with Arabic script to read Arabic text\n",
        "- Easier debugging by eliminating right-to-left display issues\n",
        "\n",
        "Additionally, some Arabic NLP systems and resources use these transliteration schemes internally.\n",
        "\n",
        "The first utility we provide to convert to/from different encodings is the [`CharMapper`](https://camel-tools.readthedocs.io/en/latest/api/utils/charmap.html). In the example below, we use a `CharMapper` to convert text in Arabic script to a Buckwalter encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jzBCQ8A088w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fda0be-6d3f-4068-ff92-dbb9b5981519"
      },
      "source": [
        "from camel_tools.utils.charmap import CharMapper\n",
        "\n",
        "sentence = \"ذهبت إلى المكتبة.\"\n",
        "print(sentence)\n",
        "\n",
        "ar2bw = CharMapper.builtin_mapper('ar2bw')\n",
        "\n",
        "sent_bw = ar2bw(sentence)\n",
        "print(sent_bw)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ذهبت إلى المكتبة.\n",
            "*hbt <lY Almktbp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ZesZuWKzIR"
      },
      "source": [
        "`CharMapper` is a very flexible component that can be used for many other types of text transformations and can be initialized with any character-to-string mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zHN9cNP088m"
      },
      "source": [
        "### Unicode Normalization\n",
        "\n",
        "Text in Python 3 is represented as a series of [Unicode](https://unicode.org/standard/standard.html) characters. Some characters have different variants, for example the character 'ع' can also be represented as 'ﻊ', 'ﻋ', 'ﻌ' depending on where it appears in a word. For all these forms, we would call 'ع' their canonical form. Furthermore, some Unicode characters are a composition of multiple characters and in some cases can comprise entire phrases. For example, the character 'ﰷ' is a single Unicode character that represents the composition of 'ك' and 'ا'. Unicode decomposition is the process of splitting these composed characters into their individual constituents.\n",
        "\n",
        "These variants and composed forms are generally used for display purposes and are problematic in text processing tasks. To convert Unicode text into something more suitable for text processing, we provide the [`normalize_unicode`](https://camel-tools.readthedocs.io/en/latest/api/utils/normalize.html#camel_tools.utils.normalize.normalize_unicode) function.\n",
        "\n",
        "In the example below, we demonstrate a more extreme example of how `normalize_unicode` converts a composed character into its decomposed form.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsR9wuFT088o",
        "outputId": "b876b856-997e-4e3d-e429-546f0ea5a460"
      },
      "source": [
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "\n",
        "sentence = 'ﷺ'\n",
        "print(sentence)\n",
        "\n",
        "sent_norm = normalize_unicode(sentence)\n",
        "print(sent_norm)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ﷺ\n",
            "صلى الله عليه وسلم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbdr5KrDlQIY"
      },
      "source": [
        "**NOTE:** It is advised to run this function on all text prior to any further preprocessing or use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUJ8IaJo088q"
      },
      "source": [
        "### Orthographic Normalization\n",
        "\n",
        "It is common for Arabic speakers to use shortcuts when typing Arabic text.\n",
        "For example, the different variants of the letter alef ('ا', 'آ', 'أ', 'إ') may be typed as just 'ا'. Some of these substitutions can just be the result of typos. The presence of these variations can cause data sparsity and are usually normalized to a single form. Orthographic normalization is the process of converting letter variants or visually similar letters into a single form.\n",
        "\n",
        "We provide a collection of orthographic normalization functions in CAMeL Tools. The example below demonstrates a few of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVN6ZsVV088r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3b6e5c-ec34-4d97-9f67-12cd790fe945"
      },
      "source": [
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "\n",
        "sentence = \"هل ذهبت إلى المكتبة؟\"\n",
        "print(sentence)\n",
        "\n",
        "# Normalize alef variants to 'ا'\n",
        "sent_norm = normalize_alef_ar(sentence)\n",
        "print(sent_norm)\n",
        "\n",
        "# Normalize alef maksura 'ى' to yeh 'ي'\n",
        "sent_norm = normalize_alef_maksura_ar(sent_norm)\n",
        "print(sent_norm)\n",
        "\n",
        "# Normalize teh marbuta 'ة' to heh 'ه'\n",
        "sent_norm = normalize_teh_marbuta_ar(sent_norm)\n",
        "print(sent_norm)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "هل ذهبت إلى المكتبة؟\n",
            "هل ذهبت الى المكتبة؟\n",
            "هل ذهبت الي المكتبة؟\n",
            "هل ذهبت الي المكتبه؟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rawo1cAJPCOC"
      },
      "source": [
        "The example above performs orthographic normalization on Arabic script. We provide variants of these functions for other encoding schemes as well (e.g. Buckwalter, Safe Buckwalter, etc). See [here](https://camel-tools.readthedocs.io/en/latest/api/utils/normalize.html) for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhD5hMgA088t"
      },
      "source": [
        "## Dediacritization\n",
        "\n",
        "Dediacritization is the process of removing Arabic diacritical marks. Diacritics increase data sparsity and so most Arabic NLP techniques ignore them. The example below shows how diacritics can be removed from Arabic text using the [`dediac_ar`](https://camel-tools.readthedocs.io/en/latest/api/utils/dediac.html#camel_tools.utils.dediac.dediac_ar) function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np_JWmFq088u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16ee6af-8927-42e6-9009-35e42ae83f8f"
      },
      "source": [
        "from camel_tools.utils.dediac import dediac_ar\n",
        "\n",
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_dediac = dediac_ar(sentence)\n",
        "print(sent_dediac)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "هل ذهبت إلى المكتبة؟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2I9gP03CHrR"
      },
      "source": [
        "We provide dediacritization functions for other encoding schemes (eg. Buckwalter, Safe Buckwalter, etc) which you can learn more about [here](https://camel-tools.readthedocs.io/en/latest/api/utils/dediac.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQw096Uw088u"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Dr0J4W_-Bp"
      },
      "source": [
        "Some CAMeL Tools components expect text to be pretokenized by whitespace and punctuation. This means that an input sentence should be an array of words and punctuation instead of a single string. This is to preserve the alignment of inputs and outputs.\n",
        "\n",
        "Python does provide the `split()` method to tokenize words by whitespace but it doesn't seperate punctuation from words. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqLIIGGim5nm",
        "outputId": "11d858ed-c5ca-4a33-e7bb-a24f514b6d9a"
      },
      "source": [
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_split = sentence.split()\n",
        "print(sent_split)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "['هَلْ', 'ذَهَبْتَ', 'إِلَى', 'المَكْتَبَةِ؟']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sju7VQ6m3h5"
      },
      "source": [
        "To split by whitespace and seperate punctuation, we currently provide the utility function [`simple_word_tokenize`](https://camel-tools.readthedocs.io/en/latest/api/tokenizers/word.html#camel_tools.tokenizers.word.simple_word_tokenize). The example below is similar to the one above, but this time we use `simple_word_tokenize()` instead of `split()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxbPNRCU088v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e242010-6eef-4b4e-a28a-1acdd3613c08"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "\n",
        "sentence = \"هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\"\n",
        "print(sentence)\n",
        "\n",
        "sent_split = simple_word_tokenize(sentence)\n",
        "print(sent_split)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "هَلْ ذَهَبْتَ إِلَى المَكْتَبَةِ؟\n",
            "['هَلْ', 'ذَهَبْتَ', 'إِلَى', 'المَكْتَبَةِ', '؟']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AMRqZE9A8tj"
      },
      "source": [
        "**NOTE:** `simple_word_tokenize`, as the name suggests is very simple and does not do anything fancy. It will seperate all punctuation regardless of context (ie. hashtags, emails, abbreviations, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv4xaPCX088y"
      },
      "source": [
        "# Morphology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7zdJFwRDUQ1"
      },
      "source": [
        "CAMeL Tools provides a powerful morphological analysis, generation and reinflection system. This system is powered by [morphological databases](https://camel-tools.readthedocs.io/en/latest/api/morphology/database.html#databases) that are comprised of lexicons and compatibility tables. See [here](https://www.aclweb.org/anthology/W18-5816.pdf) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si9YcVbY088z"
      },
      "source": [
        "## Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7JXVSeA8-rr"
      },
      "source": [
        "Morphological analysis is the process of generating all possible readings (analyses) of a given word out of context. All analyses are generated from the undiacritized form of the input word. Each of these analyses is defined by a set lexical and morphological features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtzSgEBl088z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45a67f9-c994-4172-eeb6-7d5662aa65ec"
      },
      "source": [
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.analyzer import Analyzer\n",
        "\n",
        "# First, we need to load a morphological database.\n",
        "# Here, we load the default database which is used for analyzing\n",
        "# Modern Standard Arabic.\n",
        "#db = MorphologyDB.builtin_db()\n",
        "\n",
        "analyzer = Analyzer(db)\n",
        "\n",
        "analyses = analyzer.analyze('وَسَيَكْتُبُونَها')\n",
        "\n",
        "for analysis in analyses:\n",
        "    print(analysis, '\\n')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'diac': 'وَسَيَكْتُبُونَها', 'lex': 'كَتَب', 'bw': 'وَ/PART+سَ/FUT_PART+يَ/IV3MP+كْتُب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': '[part.]_+_will_+_they_(people)+write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_part', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'caphi': 'w_a_s_a_y_a_k_t_u_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيَكْتُبُونَها', 'd2tok': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_lex_logprob': -3.648503, 'num': 'p', 'ud': 'PART+AUX+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يَ+_كْتُب_+ُونَ_+ها', 'pattern': 'وَسَيَ1ْ2ُ3ُونَها', 'lex_logprob': -3.648503, 'atbtok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd1seg': 'وَ+_سَيَكْتُبُونَها', 'stemcat': 'IV', 'stem': 'كْتُب', 'stemgloss': 'write'} \n",
            "\n",
            "{'diac': 'وَسَيَكْتُبُونَها', 'lex': 'كَتَب', 'bw': 'وَ/CONJ+سَ/FUT_PART+يَ/IV3MP+كْتُب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': 'and_+_will_+_they_(people)+write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_conj', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'caphi': 'w_a_s_a_y_a_k_t_u_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيَكْتُبُونَها', 'd2tok': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_lex_logprob': -3.648503, 'num': 'p', 'ud': 'CCONJ+ADP+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يَ+_كْتُب_+ُونَ_+ها', 'pattern': 'وَسَيَ1ْ2ُ3ُونَها', 'lex_logprob': -3.648503, 'atbtok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd1seg': 'وَ+_سَيَكْتُبُونَها', 'stemcat': 'IV', 'stem': 'كْتُب', 'stemgloss': 'write'} \n",
            "\n",
            "{'diac': 'وَسَيُكْتِبُونَها', 'lex': 'أَكْتَب', 'bw': 'وَ/SUB_CONJ+سَ/FUT_PART+يُ/IV3MP+كْتِب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': 'while_+_will_+_they_(people)+dictate;make_write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_sub', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'caphi': 'w_a_s_a_y_u_k_t_i_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيُكْتِبُونَها', 'd2tok': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_lex_logprob': -99.0, 'num': 'p', 'ud': 'CCONJ+ADP+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يُ+_كْتِب_+ُونَ_+ها', 'pattern': 'وَسَيُ1ْ2ِ3ُونَها', 'lex_logprob': -99.0, 'atbtok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd1seg': 'وَ+_سَيُكْتِبُونَها', 'stemcat': 'IV_yu', 'stem': 'كْتِب', 'stemgloss': 'dictate;make_write'} \n",
            "\n",
            "{'diac': 'وَسَيَكْتُبُونَها', 'lex': 'كَتَب', 'bw': 'وَ/SUB_CONJ+سَ/FUT_PART+يَ/IV3MP+كْتُب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': 'while_+_will_+_they_(people)+write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_sub', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'caphi': 'w_a_s_a_y_a_k_t_u_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيَكْتُبُونَها', 'd2tok': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يَكْتُبُونَها', 'pos_lex_logprob': -3.648503, 'num': 'p', 'ud': 'CCONJ+ADP+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يَ+_كْتُب_+ُونَ_+ها', 'pattern': 'وَسَيَ1ْ2ُ3ُونَها', 'lex_logprob': -3.648503, 'atbtok': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يَكْتُبُونَ_+ها', 'd1seg': 'وَ+_سَيَكْتُبُونَها', 'stemcat': 'IV', 'stem': 'كْتُب', 'stemgloss': 'write'} \n",
            "\n",
            "{'diac': 'وَسَيُكْتِبُونَها', 'lex': 'أَكْتَب', 'bw': 'وَ/CONJ+سَ/FUT_PART+يُ/IV3MP+كْتِب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': 'and_+_will_+_they_(people)+dictate;make_write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_conj', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'caphi': 'w_a_s_a_y_u_k_t_i_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيُكْتِبُونَها', 'd2tok': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_lex_logprob': -99.0, 'num': 'p', 'ud': 'CCONJ+ADP+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يُ+_كْتِب_+ُونَ_+ها', 'pattern': 'وَسَيُ1ْ2ِ3ُونَها', 'lex_logprob': -99.0, 'atbtok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd1seg': 'وَ+_سَيُكْتِبُونَها', 'stemcat': 'IV_yu', 'stem': 'كْتِب', 'stemgloss': 'dictate;make_write'} \n",
            "\n",
            "{'diac': 'وَسَيُكْتِبُونَها', 'lex': 'أَكْتَب', 'bw': 'وَ/PART+سَ/FUT_PART+يُ/IV3MP+كْتِب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS', 'gloss': '[part.]_+_will_+_they_(people)+dictate;make_write+it;them;her', 'pos': 'verb', 'prc3': '0', 'prc2': 'wa_part', 'prc1': 'sa_fut', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '3fs_dobj', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 'p', 'd3seg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'caphi': 'w_a_s_a_y_u_k_t_i_b_uu_n_a_h_aa', 'd1tok': 'وَ+_سَيُكْتِبُونَها', 'd2tok': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_logprob': -1.023208, 'd3tok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd2seg': 'وَ+_سَ+_يُكْتِبُونَها', 'pos_lex_logprob': -99.0, 'num': 'p', 'ud': 'PART+AUX+VERB+PRON', 'gen': 'm', 'catib6': 'PRT+PRT+VRB+NOM', 'root': 'ك.ت.ب', 'bwtok': 'وَ+_سَ+_يُ+_كْتِب_+ُونَ_+ها', 'pattern': 'وَسَيُ1ْ2ِ3ُونَها', 'lex_logprob': -99.0, 'atbtok': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'atbseg': 'وَ+_سَ+_يُكْتِبُونَ_+ها', 'd1seg': 'وَ+_سَيُكْتِبُونَها', 'stemcat': 'IV_yu', 'stem': 'كْتِب', 'stemgloss': 'dictate;make_write'} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3KV4vNDDtG8"
      },
      "source": [
        "Let's examine just one of the output analyses generated from another word ('وسيكتبونها') to get a better idea of the verious morphological and lexical features that we provide. We provide a [handy reference page](https://camel-tools.readthedocs.io/en/latest/reference/camel_morphology_features.html) that explains what these features are and what their values mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcfmX4eZ088z"
      },
      "source": [
        "```\n",
        "diac: وَسَيَكْتُبُونَها\n",
        "lex: كَتَب-u_1\n",
        "bw: وَ/CONJ+سَ/FUT_PART+يَ/IV3MP+كْتُب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS\n",
        "gloss: and_+_will_+_they_(people)+write+it;them;her\n",
        "pos: verb\n",
        "prc3: 0\n",
        "prc2: wa_conj\n",
        "prc1: sa_fut\n",
        "prc0: 0\n",
        "per: 3\n",
        "asp: i\n",
        "vox: a\n",
        "mod: i\n",
        "stt: na\n",
        "cas: na\n",
        "enc0: 3fs_dobj\n",
        "rat: n\n",
        "source: lex\n",
        "form_gen: m\n",
        "form_num: p\n",
        "pattern: وَسَيَ1ْ2ُ3ُونَها\n",
        "root: ك.ت.ب\n",
        "catib6: PRT+PRT+VRB+NOM\n",
        "ud: CONJ+AUX+VERB+PRON\n",
        "d1seg: وَ+_سَيَكْتُبُونَها\n",
        "d1tok: وَ+_سَيَكْتُبُونَها\n",
        "atbseg: وَ+_سَ+_يَكْتُبُونَ_+ها\n",
        "d3seg: وَ+_سَ+_يَكْتُبُونَ_+ها\n",
        "d2seg: وَ+_سَ+_يَكْتُبُونَها\n",
        "d2tok: وَ+_سَ+_يَكْتُبُونَها\n",
        "atbtok: وَ+_سَ+_يَكْتُبُونَ_+ها\n",
        "d3tok: وَ+_سَ+_يَكْتُبُونَ_+ها\n",
        "bwtok: وَ+_سَ+_يَ+_كْتُب_+ُونَ_+ها\n",
        "pos_lex_logprob: -3.648503\n",
        "caphi: w_a_s_a_y_a_k_t_u_b_uu_n_a_h_aa\n",
        "pos_logprob: -1.023208\n",
        "gen: m\n",
        "lex_logprob: -3.648503\n",
        "num: p\n",
        "stem: كْتُب\n",
        "stemgloss: write\n",
        "stemcat: IV\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfO9gxQy0880"
      },
      "source": [
        "## Generation\n",
        "\n",
        "Generation is the process of inflecting a lemma\n",
        "for a set of morphological features. The example below generates all the possible inflections for the lemma 'مُوَظَّف' (employee) that are masculine plural nouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESM-zdQ50881"
      },
      "source": [
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.generator import Generator\n",
        "\n",
        "# We need to indicate that the database we are loading will be\n",
        "# used for generation.\n",
        "#db = MorphologyDB.builtin_db(flags='g')\n",
        "\n",
        "generator = Generator(db)\n",
        "\n",
        "word = 'اِسْتَخْدَمَ'\n",
        "features = {\n",
        "    'pos': 'verb',\n",
        "    'asp': 'p',\n",
        "    'per': '2',\n",
        "    'num': 's',\n",
        "    'gen': 'm'\n",
        "}\n",
        "\n",
        "analyses = generator.generate(lemma, features)\n",
        "\n",
        "# Extract and print unique diacritizations from generated analyses\n",
        "for diac in set([a['diac'] for a in analyses]):\n",
        "    print(diac)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyses"
      ],
      "metadata": {
        "id": "rGli-7rC5kTh",
        "outputId": "70c2525f-bcf2-4d53-f64f-5a4f41ba8926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHa3zXmnFl15"
      },
      "source": [
        "**NOTE:** `'pos'` is the only *required* feature that needs to be specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z45XwXQ0881"
      },
      "source": [
        "## Reinflection\n",
        "\n",
        "Reinflection is the process of converting a given word in any form to a different form (i.e. tense, gender, etc). The CAMeL Tools reinflector works similar to the generator except that the word doesn't have to be a lemma and it is not have to be restricted to a specific `'pos'`. The example below reinflects the word 'شوارع' (streets) into its dual form with a proclitic 'ب'."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.reinflector import Reinflector\n",
        "\n",
        "# We need to indicate that the database we are loading will be\n",
        "# used for reinflection.\n",
        "#db = MorphologyDB.builtin_db(flags='r')\n",
        "\n",
        "reinflector = Reinflector(db)\n",
        "\n",
        "word = 'كَتَب'\n",
        "features = {\n",
        "    'pos': 'verb',\n",
        "    'bw': 'وَ/PART+سَ/FUT_PART+يُ/IV3MP+كْتِب/IV+ُونَ/IVSUFF_SUBJ:MP_MOOD:I+ها/IVSUFF_DO:3FS',\n",
        "    'bwtok': 'وَ+_سَ+_يُ+_كْتِب_+ُونَ_+ها'}\n",
        "\n",
        "analyses = reinflector.reinflect(word, features)\n",
        "\n",
        "# Extract and print unique diacritizations from reinflected analyses\n",
        "for diac in set([a['diac'] for a in analyses]):\n",
        "    print(diac)"
      ],
      "metadata": {
        "id": "KxvcstWPCL2M",
        "outputId": "b633b64e-3f27-435d-e665-c2ce0715ec53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "كَتَبَ\n",
            "كُتِبَ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAffYSc60881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0d09f2-c557-426b-90fb-83598ed501e6"
      },
      "source": [
        "from camel_tools.morphology.database import MorphologyDB\n",
        "from camel_tools.morphology.reinflector import Reinflector\n",
        "\n",
        "# We need to indicate that the database we are loading will be\n",
        "# used for reinflection.\n",
        "#db = MorphologyDB.builtin_db(flags='r')\n",
        "\n",
        "reinflector = Reinflector(db)\n",
        "\n",
        "word = 'قال'\n",
        "features = {\n",
        "    'pos': 'verb',\n",
        "    'asp': 'i',\n",
        "    'vox': 'a',\n",
        "    'mod': 'i',\n",
        "    'per': '2',\n",
        "    'num': 's',\n",
        "    'gen': 'm',\n",
        "    'prc0':'0',\n",
        "    'prc1':'0',\n",
        "    'prc2':'0',\n",
        "    'enc0':'3ms_dobj',\n",
        "    'prc1': 'li_prep',\n",
        "    #'enc0': '1s_pron'\n",
        "}\n",
        "\n",
        "analyses = reinflector.reinflect(word, features)\n",
        "\n",
        "# Extract and print unique diacritizations from reinflected analyses\n",
        "for diac in set([a['diac'] for a in analyses]):\n",
        "    print(diac)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "لِتَقُله\n",
            "لِتَقُوله\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTXsWdA0882"
      },
      "source": [
        "# Disambiguation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQgWp4XiGIUT"
      },
      "source": [
        "Disambiguation is the process of determining what is the most likely analysis of a word in a given context. In CAMeL Tools, disambiguation is the backbone for many Arabic NLP tasks such as diacritization, POS tagging and morphological tokenization. At the moment, we provide a disambiguator that uses a Maximum Likelihood Estimation model, [`MLEDisambiguator`](https://camel-tools.readthedocs.io/en/latest/api/disambig/mle.html#camel_tools.disambig.mle.MLEDisambiguator). More advanced disambiguation techniques will be added to CAMeL Tools in the near future.\n",
        "\n",
        "In the example below, we use a disambiguator to compute the most likely diacritization, POS tag, and lemma for all words in a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRic4ub10888",
        "outputId": "f7f45a8c-0f12-4de8-90f0-47fc6b6a513d"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "\n",
        "mle = MLEDisambiguator.pretrained()\n",
        "\n",
        "# The disambiguator expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('نجح بايدن في الانتخابات')\n",
        "\n",
        "disambig = mle.disambiguate(sentence)\n",
        "\n",
        "# For each disambiguated word d in disambig, d.analyses is a list of analyses\n",
        "# sorted from most likely to least likely. Therefore, d.analyses[0] would\n",
        "# be the most likely analysis for a given word. Below we extract different\n",
        "# features from the top analysis of each disambiguated word into seperate lists.\n",
        "diacritized = [d.analyses[0].analysis['diac'] for d in disambig]\n",
        "pos_tags = [d.analyses[0].analysis['pos'] for d in disambig]\n",
        "lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
        "\n",
        "# Print the combined feature values extracted above\n",
        "for triplet in zip(diacritized, pos_tags, lemmas):\n",
        "    print(triplet)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('نَجَحَ', 'verb', 'نَجَح')\n",
            "('بايدن', 'noun_prop', 'بايدن')\n",
            "('فِي', 'prep', 'فِي')\n",
            "('الاِنْتِخاباتِ', 'noun', 'ٱِنْتِخاب')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTRdw4g0888"
      },
      "source": [
        "# Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYy16RApKQX"
      },
      "source": [
        "The disambiguation example above is not very readable due to the amount of list and dictionary dereferencing just to extract a feature.\n",
        "Additionally, for general tagging applications, extracting a feature from the top analysis is not sufficient.\n",
        "There are some edge cases that need to be handled. For example, a word could return no analyses at all if the word isn't in the morphological database. We would then need to check the result of each disambiguated word to see if an analysis exists and then output some default value if not. Furthermore, the default value would differ depending on what feature we are tagging.\n",
        "\n",
        "To handle these edge cases and to simplify the usage of a disambiguator, we provide the [`DefaultTagger`](https://camel-tools.readthedocs.io/en/latest/api/tagger/default.html#camel_tools.tagger.default.DefaultTagger) class which wraps a disambiguator and handles edge cases by providing sensible default values. The example below shows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToGeNYB0889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4e767c-1f73-4d65-cd9d-5fd36d1c45f3"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tagger.default import DefaultTagger\n",
        "\n",
        "mle = MLEDisambiguator.pretrained()\n",
        "tagger = DefaultTagger(mle, 'pos')\n",
        "\n",
        "# The tagger expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('نجح بايدن في الانتخابات')\n",
        "\n",
        "pos_tags = tagger.tag(sentence)\n",
        "\n",
        "print(pos_tags)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['verb', 'noun_prop', 'prep', 'noun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uuXeQjd0889"
      },
      "source": [
        "# Morphological Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeKhpPVsI69"
      },
      "source": [
        "Previously we discussed word tokenization as the process of splitting sentences by whitspace and seperating punctuation. Another type of tokenization is morphological tokenization whereby Arabic words are split into component prefixes, stems, and suffixes.\n",
        "\n",
        "There are also different tokenization schemes for Arabic each having different use cases.\n",
        "\n",
        "We provide the [`MorphologicalTokenizer`](https://camel-tools.readthedocs.io/en/latest/api/tokenizers/morphological.html#camel_tools.tokenizers.morphological.MorphologicalTokenizer) class to tokenize words in different schemes. It behaves very much like the `DefaultTagger` in that it uses a disambiguator to first disambiguate words and then extracts a particular tokenization feature, but it has the following differences:\n",
        "\n",
        "- While the `DefaultTagger` produces exactly one output for each input word, the `MorphologicalTokenizer` might produce multiple output tokens.\n",
        "-  The `MorphologicalTokenizer` can be configured to produce diacritized and undiacritized output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upf_eeVE0889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7243971-3689-4897-da5f-e9be817c2e87"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "# The tokenizer expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('فتنفست الصعداء')\n",
        "print(sentence)\n",
        "\n",
        "# Load a pretrained disambiguator to use with a tokenizer\n",
        "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "\n",
        "# Without providing additional arguments, the tokenizer will output undiacritized\n",
        "# morphological tokens for each input word delimited by an underscore.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok')\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# By specifying `split=True`, the morphological tokens are output as seperate\n",
        "# strings.\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# We can output diacritized tokens by setting `diac=True`\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['فتنفست', 'الصعداء']\n",
            "['ف+_تنفست', 'ال+_صعداء']\n",
            "['ف+', 'تنفست', 'ال+', 'صعداء']\n",
            "['فَ+', 'تَنَفَّسَت', 'ال+', 'صُعَداءَ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaxVSNFH5QHi"
      },
      "source": [
        "We can see a list of supported tokenization schemes by calling the `mle.tok_feats()`.\n",
        "\n",
        "**NOTE:** Different morphological databases will have differing support for each of these tokenization schemes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb9dvHok43XH",
        "outputId": "47d7fbca-1696-4e0d-ff05-ae9632c8b3f5"
      },
      "source": [
        "print(list(mle.tok_feats()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bwtok', 'd1tok', 'd1seg', 'd3tok', 'd2seg', 'd3seg', 'd2tok', 'atbtok', 'atbseg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7hm9JjJ40u8"
      },
      "source": [
        "Try running the previous example using a different scheme than 'd3tok' to see how the output tokenization might vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GMeBGri088-"
      },
      "source": [
        "# Dialect Identification\n",
        "\n",
        "We provide a pretrained dialect identification system that can distinguish between 25 city dialects as well as Modern Standard Arabic. The model can be accessed using the [`DialectIdentifier`](https://camel-tools.readthedocs.io/en/latest/api/dialectid.html#camel_tools.dialectid.DialectIdentifier) class. In addition to city dialects, we can provide results aggregated by region and by country. While these agregated results are less fine-grained, they tend to be more accurate.\n",
        "\n",
        "The example below illustrates how `DialectIdentifier` can be used to predict the dialects of given sentences by city, country and region.\n",
        "\n",
        "**NOTE:** You may get some warnings when running the example below in Colab. These can be safely ignored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "9YpWxBAm088-",
        "outputId": "3e557c67-a8d3-4eb5-fd5c-1066de029fbd"
      },
      "source": [
        "from camel_tools.dialectid import DialectIdentifier\n",
        "\n",
        "did = DialectIdentifier.pretrained()\n",
        "\n",
        "sentences = [\n",
        "    'مال الهوى و مالي شكون اللي جابني ليك  ما كنت انايا ف حالي بلاو قلبي يانا بيك',\n",
        "    'بدي دوب قلي قلي بجنون بحبك انا مجنون ما بنسى حبك يوم'\n",
        "]\n",
        "\n",
        "predictions = did.predict(sentences, 'city')\n",
        "print([p.top for p in predictions])\n",
        "\n",
        "predictions = did.predict(sentences, 'country')\n",
        "print([p.top for p in predictions])\n",
        "\n",
        "predictions = did.predict(sentences, 'region')\n",
        "print([p.top for p in predictions])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4d8db62cfb4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialectid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDialectIdentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialectIdentifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sentences = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/camel_tools/dialectid/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialectid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInvalidDataSetError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialectid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedModelError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialectid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIDModel6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialectid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIDModel26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/camel_tools/dialectid/model6.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiIPcZcx088_"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "We provide a pretrained [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) system that has been trained using a combination of multiple data sets. The model can be accessed using the [`SentimentAnalyzer`](https://camel-tools.readthedocs.io/en/latest/api/sentiment.html#camel_tools.sentiment.SentimentAnalyzer) class and it outputs one of three sentiment tags for each sentence: `'positive'`, `'negative'` and `'neutral'`. The example below demonstrates how it can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWSDfRYd088_"
      },
      "source": [
        "from camel_tools.sentiment import SentimentAnalyzer\n",
        "\n",
        "sa = SentimentAnalyzer.pretrained()\n",
        "\n",
        "sentences = [\n",
        "    'أنا بخير',\n",
        "    'أنا لست بخير'\n",
        "]\n",
        "\n",
        "sentiments = sa.predict(sentences)\n",
        "\n",
        "print(sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jo6RMNz088_"
      },
      "source": [
        "# Named Entitiy Recognition\n",
        "\n",
        "CAMeL Tools comes with an easy-to-use, pretrained [named-entitity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)  (NER) system that can be accessed using the [`NERecognizer`](https://camel-tools.readthedocs.io/en/latest/api/ner.html#camel_tools.ner.NERecognizer) class.\n",
        "\n",
        "For each token in an input sentence, `NERecognizer` outputs a label that indicates the type of named-entity.The system outputs one of the following labels for each token: `'B-LOC'`, `'B-ORG'`, `'B-PERS'`, `'B-MISC'`, `'I-LOC'`, `'I-ORG'`, `'I-PERS'`, `'I-MISC'`, `'O'`.\n",
        "Named-entites can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous).\n",
        "\n",
        "Labels beginning with `B` indicate that their corresponding tokens are the begininging of a multi-word named-entity or is a single-token named-entity'. Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word named-entity. Words that aren't named-entities are given the `'O'` label.\n",
        "\n",
        "The example below illustrates how `NERecognizer` can be used to label named-entities in a given sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBGDtiY088_"
      },
      "source": [
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.ner import NERecognizer\n",
        "\n",
        "ner = NERecognizer.pretrained()\n",
        "\n",
        "# NERecognizer expects pre-tokenized text\n",
        "sentence = simple_word_tokenize('إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع.')\n",
        "\n",
        "labels = ner.predict_sentence(sentence)\n",
        "\n",
        "# Print each token paired with it's NER label\n",
        "print(list(zip(sentence, labels)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}